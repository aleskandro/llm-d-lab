apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: autoscaling-test-wva-
  namespace: experiment-01
spec:
  params:
    - name: target-url
      value: 'openshift-ai-inference-openshift-ai-inference.openshift-ingress.svc.cluster.local'
    - name: model
      value: meta-llama/Llama-3.1-8B
    - name: reset-llm-d
      value: 'true'
    - name: namespace
      value: experiment-01
    - name: test-name
      value: WVA
    - name: scaled-object
      value: |
        apiVersion: keda.sh/v1alpha1
        kind: ScaledObject
        metadata:
          name: vllme-deployment-scaler
          namespace: experiment-01
          labels:
            app: vllme-deployment
            scaler: keda-workload-variant-autoscaler
        spec:
          scaleTargetRef:
            apiVersion: apps/v1                 
            kind: Deployment                   
            name: ms-inference-scheduling-llm-d-modelservice-decode
          pollingInterval: 10  
          cooldownPeriod: 30
          initialCooldownPeriod: 30
          minReplicaCount: 1  
          maxReplicaCount: 12
          fallback:
            failureThreshold: 3   
            replicas: 1     
            behavior: 'currentReplicasIfHigher'
          advanced:
            restoreToOriginalReplicaCount: false
            horizontalPodAutoscalerConfig:
              name: ms-inference-scheduling-llm-d-modelservice-decode-hpa
              behavior:
                scaleDown:
                  stabilizationWindowSeconds: 60
                  policies:
                    - type: Percent
                      value: 100         
                      periodSeconds: 30  
                    - type: Pods
                      value: 1      
                      periodSeconds: 15
                scaleUp:
                  stabilizationWindowSeconds: 0
                  policies:
                    - type: Percent
                      value: 100          
                      periodSeconds: 30   
                    - type: Pods
                      value: 1          
                      periodSeconds: 15
          triggers:
            - type: prometheus
              name: inferno-desired-replicas

              authenticationRef:
                name: keda-trigger-auth-prometheus
              metadata:
                serverAddress: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091'
                query: |
                  inferno_desired_replicas{
                    variant_name='ms-inference-scheduling-llm-d-modelservice-decode',
                    exported_namespace='experiment-01'
                  }
                threshold: '1'
                activationThreshold: '0'
                metricType: 'AverageValue'
                authModes: 'bearer'
                unsafeSsl: 'true'
  pipelineRef:
    name: wva-incremental-stepped-load-test
  taskRunTemplate:
    podTemplate:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
    serviceAccountName: pipeline
  timeouts:
    finally: 1h0m0s
    pipeline: 12h0m0s
    tasks: 4h0m0s
  workspaces:
    - name: shared-workspace
      volumeClaimTemplate:
        metadata:
          creationTimestamp: null
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 32Gi
        status: {}