# Set the values for the Model Service Helm chart deployment
- op: replace
  path: /spec/sources/1/helm/valuesObject
  value:
    modelArtifacts:
      uri: "hf://meta-llama/Llama-3.1-8B"
      size: 32Gi
      authSecretName: "llm-d-hf-token"
      name: "meta-llama/Llama-3.1-8B"

    decode:
      monitoring:
        podmonitor:
          enabled: true
      containers:
        - name: "vllm"
          modelCommand: vllmServe
          args:
            - "--kv-transfer-config"
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
            - '--max-num-seqs'
            - '64'
            - '--max-model-len'
            - '4096'
            - '--gpu-memory-utilization'
            - '0.90'
            - '--max-num-batched-tokens'
            - '8192'
            - '--disable-log-requests'
            - '--dtype'
            - 'bfloat16'
            - "--disable-uvicorn-access-log"
          image: ghcr.io/llm-d/llm-d-cuda:v0.3.1
          env:
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            - name: UCX_TLS
              value: "cuda_ipc,cuda_copy,tcp"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_NIXL_SIDE_CHANNEL_PORT
              value: "5557"
            - name: VLLM_LOGGING_LEVEL
              value: INFO
          ports:
            - containerPort: 5557
              protocol: TCP
            - containerPort: 8200
              name: metrics
              protocol: TCP
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          mountModelVolume: true
          volumeMounts:
            - name: metrics-volume
              mountPath: /.config
            - name: torch-compile-cache
              mountPath: /.cache
            - name: cache-volume
              mountPath: /tmp/vllm
          startupProbe:
            httpGet:
              path: /v1/models
              port: 8200
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 120
          livenessProbe:
            httpGet:
              path: /health
              port: 8200
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8200
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 3
      volumes:
        - name: metrics-volume
          emptyDir: { }
        - name: torch-compile-cache
          emptyDir:
            medium: Memory
        - name: cache-volume
          emptyDir:
            medium: Memory

      tolerations: &tolerations
        - effect: NoSchedule
          key: benchmark.llm-d.ai/test-gpu-amd64
          operator: Exists
      replicas: 1

    multinode: false
    prefill:
      create: false
