# Set the values for the GAIE Helm chart deployment
- op: replace
  path: /spec/sources/0/helm/valuesObject
  value:
    inferenceExtension:
      replicas: 1
      image:
        name: llm-d-inference-scheduler
        hub: ghcr.io/llm-d
        tag: v0.4.0-rc.1
        pullPolicy: Always
      extProcPort: 9002
      pluginsConfigFile: "default-plugins.yaml"
      monitoring:
        interval: "10s"
        # Service account token secret for authentication
        secret:
          name: inference-gateway-sa-metrics-reader-secret
        # Prometheus ServiceMonitor will be created when enabled for EPP metrics collection
        prometheus:
          enabled: true
    inferencePool:
      apiVersion: inference.networking.x-k8s.io/v1alpha2 # use old API version for inference
      targetPortNumber: 8000
      modelServerType: vllm
      modelServers:
        matchLabels:
          llm-d.ai/inferenceServing: "true"
    provider:
      name: istio
      istio:
        destinationRule:
          host: "llm-d-inference-scheduling-epp.experiment-01.svc.cluster.local"
          trafficPolicy:
            tls:
              mode: SIMPLE
              insecureSkipVerify: true