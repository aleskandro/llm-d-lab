wva:
  enabled: true

  image:
    repository: ghcr.io/llm-d-incubation/workload-variant-autoscaler
    tag: v0.4.1
  imagePullPolicy: Always

  metrics:
    enabled: true
    port: 8443
    secure: true
  
  reconcileInterval: 60s
    
  prometheus:
    monitoringNamespace: openshift-user-workload-monitoring
    serviceAccountName: "kube-prometheus-stack-prometheus"
    baseURL: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
    # Development security configuration (relaxed for easier development)
    tls:
      insecureSkipVerify: true   # Development: true, Production: false
      caCertPath: "/etc/ssl/certs/prometheus-ca.crt"
    caCert: |
        ########### REPLACE ME #############


# Environment variable to enable experimental hybrid-based optimization
  #  When "on", runs both capacity analyzer and model-based optimizer with arbitration
  #  When "model-only" runs model-based optimizer only
  #  When "off" or unset, runs capacity analyzer only (default, reactive mode)
  experimentalHybridOptimization: off  # Enable experimental hybrid optimization (default: off)
  scaleToZero: false  # Enable scaling variants to zero replicas (default: false)

  # Capacity-based scaling configuration
  # These thresholds determine when replicas are saturated and when to scale up
  capacityScaling:
    # Global defaults applied to all variants unless overridden
    default:
      kvCacheThreshold: 0.80      # Replica saturated if KV cache utilization >= threshold (0.0-1.0)
      queueLengthThreshold: 5     # Replica saturated if queue length >= threshold
      kvSpareTrigger: 0.1         # Scale-up if avg spare KV capacity < trigger (0.0-1.0)
      queueSpareTrigger: 3        # Scale-up if avg spare queue capacity < trigger
    
    # Per-model/namespace overrides (optional)
    # Example:
    # overrides:
    #   llm-d:
    #     modelID: "Qwen/Qwen3-0.6B"
    #     namespace: "llm-d-autoscaler"
    #     kvCacheThreshold: 0.70
    #     kvSpareTrigger: 0.35
    overrides: {}

llmd:
  namespace: experiment-01
  modelName: ms-inference-scheduling-llm-d-modelservice
  modelID: "meta-llama/Llama-3.1-8B"


va:
  enabled: true
  accelerator: L40S
  sloTpot: 10
  sloTtft: 1000

hpa:
  enabled: false
  maxReplicas: 10
  targetAverageValue: "1"

vllmService:
  enabled: false
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
