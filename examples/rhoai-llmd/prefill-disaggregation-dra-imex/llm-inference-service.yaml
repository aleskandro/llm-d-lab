apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: deepseek-r1-70b-pd
  namespace: llm-d-pd-disaggregation
  annotations:
    serving.kserve.io/enable-prometheus-metrics: "true"
    security.opendatahub.io/enable-auth: "false"
spec:
  replicas: 1
  model:
    name: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
    uri: "pvc://model-storage/models/deepseek-ai-DeepSeek-R1-Distill-Llama-70B"
  router:
    route: {}
    gateway: {}
    scheduler:
      template:
        containers:
          - name: main
            args:
              - '--pool-name'
              - deepseek-r1-70b-pd-inference-pool
              - '--pool-namespace'
              - llm-d-pd-disaggregation
              - '--pool-group'
              - inference.networking.x-k8s.io
              - '--zap-encoder'
              - console
              - '--grpc-port'
              - '9002'
              - '--grpc-health-port'
              - '9003'
              - '--secure-serving'
              - '--model-server-metrics-scheme'
              - https
              - '--model-server-metrics-https-insecure-skip-verify'
              - '--cert-path'
              - /var/run/kserve/tls
              - '--config-text'
              - |
                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                  - type: prefill-header-handler
                  - type: prefill-filter
                  - type: decode-filter
                  - type: max-score-picker
                  - type: queue-scorer
                  - type: pd-profile-handler
                    parameters:
                      threshold: 0
                schedulingProfiles:
                  - name: prefill
                    plugins:
                      - pluginRef: prefill-filter
                      - pluginRef: queue-scorer
                        weight: 1.0
                      - pluginRef: max-score-picker
                  - name: decode
                    plugins:
                      - pluginRef: decode-filter
                      - pluginRef: queue-scorer
                        weight: 1.0
                      - pluginRef: max-score-picker

  template:
    serviceAccount: ai-workload-service-account
    resourceClaims:
      - name: gpu
        resourceClaimTemplateName: imex-channel
    containers:
      - name: main
        command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
        args:
          - "--port=8001"
          - "--model=/mnt/models"
          - "--served-model-name=deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
          - "--tensor-parallel-size=4"
          - "--max-model-len=16384"
          - "--gpu-memory-utilization=0.95"
          - "--trust-remote-code"
          - "--max-num-seqs=512"
          - "--enable-ssl-refresh"
          - "--ssl-certfile=/var/run/kserve/tls/tls.crt"
          - "--ssl-keyfile=/var/run/kserve/tls/tls.key"
          - "--kv-transfer-config={\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}"
        env: # TODO: Verify these env vars
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_DEVICE
            value: "cuda"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: "spawn"
          - name: LD_LIBRARY_PATH
            value: "/usr/lib64:/opt/app-root/lib64:/usr/local/cuda/lib64"
          - name: NUMBA_CUDA_DRIVER
            value: "/usr/lib64/libcuda.so.1"
          - name: CUDA_HOME
            value: "/usr/local/cuda"
          - name: NCCL_IB_DISABLE
            value: "1"
          - name: NCCL_P2P_DISABLE
            value: "0"
          - name: NCCL_NVLS_ENABLE
            value: "1"
          - name: NCCL_DEBUG
            value: "INFO"
          - name: VLLM_LOGGING_LEVEL
            value: "INFO"
          - name: VLLM_USE_V2_BLOCK_MANAGER
            value: "1"
          - name: VLLM_ATTENTION_BACKEND
            value: "FLASHINFER"
        resources:
          limits:
            nvidia.com/gpu: "4"
          claims:
            - name: gpu
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 400
          periodSeconds: 30
          timeoutSeconds: 60
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 60
          successThreshold: 1
          failureThreshold: 60
        volumeMounts:
          - name: dshm
            mountPath: /dev/shm
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
  prefill:
    replicas: 2
    template:
      serviceAccount: ai-workload-service-account
      resourceClaims:
        - name: gpu
          resourceClaimTemplateName: imex-channel
      containers:
        - name: main
          command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
          args:
            - "--port=8000"
            - "--model=/mnt/models"
            - "--served-model-name=deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
            - "--tensor-parallel-size=4"
            - "--max-model-len=16384"
            - "--gpu-memory-utilization=0.90"
            - "--enforce-eager"
            - "--trust-remote-code"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens=16384"
            - "--max-num-seqs=256"
            - "--enable-ssl-refresh"
            - "--ssl-certfile=/var/run/kserve/tls/tls.crt"
            - "--ssl-keyfile=/var/run/kserve/tls/tls.key"
            - "--kv-transfer-config={\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}"
          env:
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_DEVICE
              value: "cuda"
            - name: VLLM_WORKER_MULTIPROC_METHOD
              value: "spawn"
            - name: LD_LIBRARY_PATH
              value: "/usr/lib64:/opt/app-root/lib64:/usr/local/cuda/lib64"
            - name: NUMBA_CUDA_DRIVER
              value: "/usr/lib64/libcuda.so.1"
            - name: CUDA_HOME
              value: "/usr/local/cuda"
            - name: NCCL_IB_DISABLE
              value: "1"
            - name: NCCL_P2P_DISABLE
              value: "0"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: NCCL_DEBUG
              value: "INFO"
            - name: VLLM_LOGGING_LEVEL
              value: "INFO"
            - name: VLLM_USE_V2_BLOCK_MANAGER
              value: "1"
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASHINFER"
          resources:
            claims:
              - name: gpu
            limits:
              nvidia.com/gpu: "4"
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 60
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 60
            successThreshold: 1
            failureThreshold: 20
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi

